# Configuration du Terminal IA Autonome
# Copiez ce fichier vers .env et modifiez selon vos besoins

# Configuration Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama2
OLLAMA_TIMEOUT=120

# Configuration des logs
LOG_LEVEL=INFO

# Configuration du cache Ollama (Phase 1: Performance)
CACHE_ENABLED=true
CACHE_TTL_HOURS=24
MAX_CACHE_SIZE_MB=500
CACHE_EVICTION=lru

# Configuration performance (Phase 1)
PARALLEL_WORKERS=auto
AGENT_MAX_STEPS=50
AGENT_MAX_DURATION=30

# Note: Pour Raspberry Pi 5, assurez-vous que Ollama est installé et en cours d'exécution
# Vous pouvez utiliser un modèle plus léger comme 'phi' ou 'mistral' pour de meilleures performances
# Le cache et l'exécution parallèle amélioreront significativement les performances
