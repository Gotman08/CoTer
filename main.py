#!/usr/bin/env python3
"""
Terminal IA Autonome
Point d'entr√©e principal de l'application
"""

import sys
import argparse
import multiprocessing
import requests
from typing import List, Tuple, Optional
from simple_term_menu import TerminalMenu
from src.terminal_interface import TerminalInterface
from src.utils.logger import setup_logger
from src.utils import HardwareOptimizer, CacheManager, OllamaManager
from config.settings import Settings
from config import CacheConfig, constants


def check_ollama_connection(host: str, timeout: int = 5) -> bool:
    """
    V√©rifie si Ollama est accessible

    Args:
        host: URL du serveur Ollama
        timeout: Timeout en secondes

    Returns:
        True si Ollama est accessible, False sinon
    """
    try:
        response = requests.get(f"{host}/api/tags", timeout=timeout)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException:
        return False


def get_available_models(host: str, timeout: int = 5) -> List[dict]:
    """
    R√©cup√®re la liste des mod√®les Ollama disponibles avec leurs informations

    Args:
        host: URL du serveur Ollama
        timeout: Timeout en secondes

    Returns:
        Liste de dictionnaires avec les infos des mod√®les
    """
    try:
        response = requests.get(f"{host}/api/tags", timeout=timeout)
        response.raise_for_status()
        data = response.json()
        return data.get('models', [])
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration des mod√®les: {e}")
        return []


def format_model_size(size_bytes: int) -> str:
    """
    Formate la taille d'un mod√®le en unit√©s lisibles

    Args:
        size_bytes: Taille en bytes

    Returns:
        Taille format√©e (ex: "4.1 GB")
    """
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} PB"


def select_ollama_model_interactive(
    host: str,
    configured_model: str,
    logger=None
) -> Tuple[Optional[str], bool]:
    """
    S√©lectionne un mod√®le Ollama de mani√®re interactive avec navigation par fl√®ches

    Args:
        host: URL du serveur Ollama
        configured_model: Mod√®le configur√© (via .env ou --model)
        logger: Logger pour les messages

    Returns:
        Tuple (mod√®le_s√©lectionn√©, mod√®le_chang√©)
        - mod√®le_s√©lectionn√©: Nom du mod√®le choisi (ou None si erreur)
        - mod√®le_chang√©: True si diff√©rent du mod√®le configur√©
    """
    print("\n" + "="*60)
    print("üîç D√âTECTION DES MOD√àLES OLLAMA")
    print("="*60)

    # V√©rifier la connexion Ollama
    if not check_ollama_connection(host):
        print(f"\n‚ùå Impossible de se connecter √† Ollama sur {host}")
        print("\nüí° V√©rifiez que:")
        print("   1. Ollama est install√© (https://ollama.ai)")
        print("   2. Le service Ollama est d√©marr√©")
        print("   3. L'URL est correcte (par d√©faut: http://localhost:11434)")
        return None, False

    # R√©cup√©rer les mod√®les disponibles
    models = get_available_models(host)

    if not models:
        print("\n‚ùå Aucun mod√®le Ollama d√©tect√©!")
        print("\nüí° Pour installer un mod√®le, utilisez:")
        print("   ollama pull llama2")
        print("   ollama pull mistral")
        print("   ollama pull codellama")
        return None, False

    # Extraire les noms de mod√®les
    model_names = [m['name'] for m in models]

    # Cas 1: Un seul mod√®le disponible
    if len(models) == 1:
        selected_model = model_names[0]
        model_size = format_model_size(models[0].get('size', 0))

        if configured_model and configured_model not in model_names:
            print(f"\n‚ö†Ô∏è  Le mod√®le configur√© '{configured_model}' n'est plus disponible")
            print(f"‚úì S√©lection automatique du seul mod√®le disponible: {selected_model} ({model_size})")
            if logger:
                logger.warning(f"Mod√®le '{configured_model}' introuvable, utilisation de '{selected_model}'")
            return selected_model, True
        else:
            print(f"\n‚úì Un seul mod√®le disponible: {selected_model} ({model_size})")
            return selected_model, False

    # Cas 2: Plusieurs mod√®les - menu interactif
    print(f"\n‚úì {len(models)} mod√®les Ollama d√©tect√©s\n")

    # V√©rifier si le mod√®le configur√© existe
    model_changed = configured_model not in model_names
    if configured_model and model_changed:
        print(f"‚ö†Ô∏è  Le mod√®le configur√© '{configured_model}' n'est plus disponible")
        print("Veuillez en s√©lectionner un autre:\n")
        if logger:
            logger.warning(f"Mod√®le '{configured_model}' introuvable")
    elif configured_model:
        print(f"‚ÑπÔ∏è  Mod√®le configur√©: {configured_model}")
        print("Vous pouvez le changer ci-dessous:\n")

    # Pr√©parer les options du menu avec tailles
    menu_options = []
    for model in models:
        name = model['name']
        size = format_model_size(model.get('size', 0))
        marker = " ‚úì" if name == configured_model else ""
        menu_options.append(f"{name} ({size}){marker}")

    # Cr√©er le menu interactif
    try:
        terminal_menu = TerminalMenu(
            menu_options,
            title="S√©lectionnez un mod√®le Ollama (‚Üë‚Üì pour naviguer, Entr√©e pour valider):",
            cursor_index=model_names.index(configured_model) if configured_model in model_names else 0
        )

        menu_index = terminal_menu.show()

        if menu_index is None:
            # Utilisateur a annul√© (Ctrl+C)
            print("\n‚ö†Ô∏è  S√©lection annul√©e")
            if configured_model in model_names:
                print(f"Utilisation du mod√®le configur√©: {configured_model}")
                return configured_model, False
            else:
                return None, False

        selected_model = model_names[menu_index]
        model_size = format_model_size(models[menu_index].get('size', 0))

        print(f"\n‚úì Mod√®le s√©lectionn√©: {selected_model} ({model_size})")

        return selected_model, selected_model != configured_model

    except Exception as e:
        print(f"\n‚ùå Erreur lors de la s√©lection: {e}")
        if configured_model in model_names:
            print(f"Utilisation du mod√®le configur√©: {configured_model}")
            return configured_model, False
        return None, False


def main():
    """Point d'entr√©e principal de l'application"""
    parser = argparse.ArgumentParser(description='Terminal IA Autonome avec Ollama')
    parser.add_argument('--debug', action='store_true', help='Activer le mode debug')
    parser.add_argument('--model', type=str, help='Mod√®le Ollama √† utiliser')
    args = parser.parse_args()

    # Configuration du logger
    logger = setup_logger(debug=args.debug)
    logger.info("D√©marrage du Terminal IA...")

    # Phase 1: Optimisation hardware
    logger.info("D√©tection et optimisation du hardware...")
    hardware_optimizer = HardwareOptimizer(logger)

    # Afficher le rapport d'optimisation
    print("\n" + hardware_optimizer.get_optimization_report())

    # Chargement de la configuration
    settings = Settings()
    if args.model:
        settings.ollama_model = args.model

    # Appliquer les optimisations hardware
    hardware_optimizer.apply_optimizations(settings)

    # Phase: V√©rification et d√©marrage automatique du serveur Ollama
    logger.info("V√©rification du serveur Ollama...")
    ollama_manager = OllamaManager(settings.ollama_host, logger)

    is_running, message = ollama_manager.ensure_server_running()

    if not is_running:
        print(f"\n‚ùå ERREUR: {message}")
        logger.error(f"Impossible de d√©marrer Ollama: {message}")
        sys.exit(1)
    else:
        print(f"‚úÖ {message}")
        logger.info(message)

    # Phase: D√©tection et s√©lection interactive des mod√®les Ollama
    selected_model, model_changed = select_ollama_model_interactive(
        host=settings.ollama_host,
        configured_model=settings.ollama_model,
        logger=logger
    )

    # V√©rifier que le mod√®le a √©t√© s√©lectionn√© avec succ√®s
    if selected_model is None:
        logger.error("Impossible de s√©lectionner un mod√®le Ollama")
        print("\n‚ùå D√©marrage annul√©: aucun mod√®le Ollama disponible")
        sys.exit(1)

    # Mettre √† jour le mod√®le s√©lectionn√©
    if model_changed:
        logger.info(f"Changement de mod√®le: {settings.ollama_model} ‚Üí {selected_model}")
    settings.ollama_model = selected_model

    # Phase 1: Initialisation du cache Ollama
    cache_config = CacheConfig()
    cache_manager = None
    if cache_config.cache_enabled:
        cache_manager = CacheManager(cache_config, logger)
        logger.info("‚úÖ Cache Ollama initialis√©")

    try:
        # Initialisation de l'interface terminal
        terminal = TerminalInterface(settings, logger, cache_manager=cache_manager)
        terminal.run()
    except KeyboardInterrupt:
        logger.info("\nArr√™t du Terminal IA...")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Erreur fatale: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    # Configuration du multiprocessing pour Windows
    # IMPORTANT: Sur Windows, 'spawn' est obligatoire pour √©viter les probl√®mes
    # Sur Linux/Mac, 'fork' est plus rapide mais 'spawn' est plus s√ªr
    try:
        # V√©rifier si la m√©thode n'est pas d√©j√† configur√©e
        current_method = multiprocessing.get_start_method(allow_none=True)
        if current_method is None:
            multiprocessing.set_start_method(constants.PARALLEL_PROCESS_START_METHOD, force=False)
        elif current_method != constants.PARALLEL_PROCESS_START_METHOD:
            # M√©thode d√©j√† configur√©e mais diff√©rente, forcer le changement
            multiprocessing.set_start_method(constants.PARALLEL_PROCESS_START_METHOD, force=True)
    except RuntimeError as e:
        # D√©j√† configur√© avec la m√™me m√©thode, continuer normalement
        pass
    except Exception as e:
        # Autre erreur lors de la configuration multiprocessing
        print(f"Attention: Impossible de configurer multiprocessing: {e}")
        print("L'application continuera avec les param√®tres par d√©faut.")

    # D√©marrer l'application
    main()
